{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paarth Tandon\\Desktop\\repos\\TrashGPT\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\Paarth Tandon\\Desktop\\repos\\TrashGPT\\.venv\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.70s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', use_fast=True)\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          'meta-llama/Llama-2-7b-hf', quantization_config=bnb_config, device_map={\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(model, \"../checkpoints/checkpoint-2000\")\n",
    "peft_model = peft_model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  it's a\n",
      "00614.800 <garnt>: very weird experience because you're just\n",
      "00616.079 <garnt>: not you're just not like your normal\n",
      "00618.000 <garnt>: self or whatever and it's like it's like\n",
      "00620.000 <garnt>: it's like it's like a very weird\n",
      "00622.000 <joey>: experience going from normal to just\n",
      "00623.760 <garnt>: just a complete stranger because you can\n",
      "00625.360 <garnt>: only talk about in certain ways yeah\n",
      "00626.799 <garnt>: yeah because like you know because of\n",
      "00628.880 <garnt>: like my upbringing like the thing that\n",
      "00630.960 <garnt>:'s a weird feeling for me is the fact\n",
      "00632.559 <joey>: that when when a girl does do something\n",
      "00634.800 <joey>: weird and like i have a joke i know\n",
      "00636.720 <joey>: that's a joke i don't know a girl and\n",
      "00638.800 <joey>: they do something weird and i say a\n",
      "00640.320 <joey>: joke i'm worried they don't think it's a\n",
      "00642.239 <joey>: joke\n",
      "00644.159 <joey>: because i'm afraid like oh yeah she's\n",
      "00646.800 <garnt>: actually taking that seriously oh are\n",
      "00648.559 <joey>: you afraid that girls are taking stuff\n",
      "00651.500 <joey>: seriously when you like make a joke\n",
      "00654.559 <joey>: oh no no no oh so so yeah so if if you\n",
      "00665.760 <joey>: say it's a joke you got to\n"
     ]
    }
   ],
   "source": [
    "prompt = ''\n",
    "# prompt = '-----\\n\\n'\n",
    "# prompt = '-----\\n\\n00000.000 <'\n",
    "# prompt = '-----\\n\\n00000.000 <connor>: so what were you boys up to this week'\n",
    "# prompt = '-----\\n\\n00000.000 <connor>: what is better, bone in or boneless chicken'\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].cuda()\n",
    "generation_output = peft_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(\n",
    "        do_sample=True, temperature=1.001, top_p=0.95, top_k=50, repetition_penalty=1.001\n",
    "    ),\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "for seq in generation_output.sequences:\n",
    "    output = tokenizer.decode(seq)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
